# 5 序列模型
## 1 循环序列模型(RNN)
### 1.1 为什么选择序列模型？
序列模型的应用领域包括语音识别、音乐生成、情感分类、DNA 序列分析、机器翻译、视频行为识别和命令实体识别等。这些问题都可以被称作使用标签数据(x,y)作为训练集的监督学习，序列问题也可以分为很多不同的类型，包括输入和输出都为序列、输入输出序列长度不同、或仅有输入或输出为序列等。
![](/第五课/1.jpg)
### 1.2 数学符号
在序列模型命名规则中，对于输入x，表示为
![](/第五课/2.png)
y<t>表示为对应输入的输出，Ty表示为输出序列长度，且1<t<Ty。
Tx表示输入序列长度，要注意也存在Tx≠Ty的情况。
对于自然语言处理中，可以使用one-hot表示法来表示词典中的每一个单词，x<t>表示为一个与词典同维度的向量，该向量中，x<t>表示的单词与词典中对应的单词位置为1，其余均为0。值得一提的是如果输入序列中出现词典之外的单词，可以使用UNK或其他字符串来表示。
对于多样本输入，类似于卷积神经网络表示方法，即(i)表示第i个样本。
### 1.3 循环神经网络模型
循环神经网络（RNN）是专门用来解决序列模型问题的。RNN模型结构如下：
![](/第五课/3.jpg)
序列模型从左到右，依次传递，此例中，Tx=Ty。x<t>到y<t>之间是隐藏神经元。a<T>会作为输入传入到第t+1个元素中。其中，a<0>一般为零向量。
RNN模型包含三类权重系数，分别是Wax、Waa、Wya，且不同元素之间同一位置共享同一权重系数。
RNN的正向传播为：
![](/第五课/4.png)
其中，g()表示激活函数，不同的问题需要使用不同的激活函数，RNN中使用的激活函数通常是tanh()；权重系数Wax的下标中，第一个下标a表示它是用来计算某个a类型的变量；第二个下标x表示Wax要乘以某个x类型的量。
为了简化表达式，可以对a<t>项进行整合：
![](/第五课/5.png)
则正向传播表达式变为：
![](/第五课/6.png)
注意，以上阐述的RNN模型是单向RNN模型，即按照从左到右顺序，单向进行，y_hat<t>只与左边的元素有关，有另外一种RNN结构是双向RNN，简称为BRNN，y_hat<t>与左右元素均有关系，将在后面小节中学到。最后在放一张更加形象的RNN示意图：
![](/第五课/7.png)
### 1.4 Backpropagation through time
在编程框架中实现RNN时，编程框架会处理反向传播自动求导，但对单向传播的运行有一个粗略的认识还是很有必要的。为了进行反向传播，先定义一个损失函数：
![](/第五课/8.png)
对于多样本所有元素的loss function为：
![](/第五课/9.png)
然后，反向传播（Backpropagation）过程就是从右到左分别计算L(y_hat,y)对参数Wa、Wy、ba、by的偏导数，这种从右到左的求导过程被称为Backpropagation through time。
![](/第五课/10.png)
### 1.5 不同类型的循环神经网络
以上介绍的例子中，Tx=Ty，但是在很多RNN模型中，Tx是不等于Ty的。根据Tx与Ty的关系，RNN模型包含以下几个类型：
![](/第五课/11.png)
![](/第五课/12.jpg)
### 1.6 语言模型和序列生成
语言模型是自然语言处理（NLP）中最基本和最重要的任务之一。使用RNN能够很好地建立需要的不同语言风格的语言模型。
对于语音识别应用中，利用语言模型可以得到翻译为不同语句的概率，选择概率最大的语句作为正确的翻译。
如何使用RNN构建语言模型？
首先，我们需要一个足够大的训练集，训练集由大量的单词语句语料库（corpus）构成。然后，对corpus的每句话进行切分词（tokenize）。做法就跟第2节介绍的一样，建立vocabulary，对每个单词进行one-hot编码。需要注意的是，每句话结束末尾，需要加上<EOS>作为语句结束符。另外，若语句中有词汇表中没有的单词，用< UNK >表示。准备好训练集并对语料库进行切分词等处理之后，接下来构建相应的RNN模型。![](/第五课/13.jpg)
语言模型的RNN结构如上图所示，a<0>和x<1>均为零向量。Softmax输出层y_hat<1>表示出现该语句第一个单词的概率，softmax输出层y_hat<2>表示在第一个单词基础上出现第二个单词的概率，即条件概率，以此类推，最后是出现< EOS >的条件概率。
单个元素的softmax loss function为：
![](/第五课/14.png)
该样本所有元素的Loss function为:
![](/第五课/15.png)
对语料库的每条语句进行RNN模型训练，最终得到的模型可以根据给出语句的前几个单词预测其余部分，将语句补充完整。
最后补充一点，整个语句出现的概率等于语句中所有元素出现的条件概率乘积。
![](/第五课/16.png)
### 1.7 对新序列采样(Sampling novel sequences)
利用训练好的RNN语言模型，可以进行新的序列采样，从而随机产生新的语句。
首先，从第一个元素y_hat<1>输出的softmax分布中随机选取一个word作为新语句的首单词。然后，y<1>作为x<2>，得到y_hat<2>的softmax分布。从中选取概率最大的word作为y<2>，继续将y<2>作为x<3>，以此类推。直到产生< EOS >结束符，则标志语句生成完毕。当然，也可以设定语句长度上限，达到长度上限即停止生成新的单词。最终，根据随机选择的首单词，RNN模型会生成一条新的语句。
以上介绍的是word level RNN，即每次生成单个word，语句由多个words构成。另外一种情况是character level RNN，即词汇表由单个英文字母或字符组成，如下所示：
![](/第五课/17.png)
对字母随机采样时，每次生成的为一个字母，字母逐渐生成单词及空格构成句子。它的优点是不会出现<UNK>的情况，缺点是计算量很大且每句话的字符数量很大，这种大的跨度不利于寻找语句前部分和后部分之间的连贯性及依赖性。
### 1.8 循环神经网络的梯度消失
一般的RNN模型每个元素受其附近元素的影响较大，而难以建立跨度较大的依赖性。对于改善深层神经网络的学习，提到梯度爆炸和梯度消失问题，即深层神经网络在执行反向传播时，梯度很难修正浅层网络的权重。
而对于梯度爆炸问题，由于指数级增长的权重可以很容易被发现，你可能会看到很多NaN，它的一个解决办法是用梯度修剪，即如果梯度向量大于某个阈值，则缩放梯度向量，保证它不会太大，即可解决梯度爆炸问题。而对于梯度消失问题不易发现且比较棘手， 通过GRU(门控循环单元)可以有效解决梯度消失问题。
### 1.9 GRU单元
RNN的隐藏层单元结构如下图所示：
![](/第五课/18.jpg)
a<t>的表达式为：
![](/第五课/19.png)
为了解决梯度消失问题，对上述单元进行修改，添加了记忆单元，构建GRU(Gated Recurrent Units)，如下图所示：
![](/第五课/20.jpg)
相应的表达式为
![](/第五课/21.png)
其中c<t-1>=a<t-1>，c<t>=a<t>，Γu意为gate。当Γu=1时，代表更新；当Γu=0时，代表记忆，保留之前的模块输出。这一点跟CNN中的ResNets的作用有点类似。因此， 能够保证RNN模型中跨度很大的依赖关系不受影响，消除梯度消失问题。
以上是简化的GRU模型，完整的GRU添加了Γr，即，表达式如下：
![](/第五课/22.png)
Γr可理解为相关性，即下一个c<t>的候选值c~<t>与c<t-1>有多大的相关性。
### 1.10 LSTM(Long short term memory)unit
LSTM是另一种更强大的解决梯度消失问题的方法。它对应的RNN隐藏层单元结构如下图所示：
![](/第五课/23.jpg)
相应的表达式为：
![](/第五课/24.png)
LSTM包含三个gates：Γu、Γf、Γo，分别对应update gate，forget gate和output gate。
最常用的版本可能是gate值不仅取决于以上，还会“偷窥”一下c<t-1>的值，这叫做"窥视孔连接"(peephole connection)，相应表达式如下：![](/第五课/25.png)
### 1.11 双向循环神经网络(BRNN)
双向循环神经网络的结构图如下：
![](/第五课/26.jpg)
BRNN对应的输出y<t>表达式为：
![](/第五课/27.png)
### 1.12 Deep RNNs
Deep RNNs是由多层RNN、GRU或LSTM单元合并在一起组成网络，其结构如下图所示：![](/第五课/28.jpg)
与DNN一样，用上标表示层数。Deep RNNs中的a[l]<t>表达式为：
![](/第五课/29.png)